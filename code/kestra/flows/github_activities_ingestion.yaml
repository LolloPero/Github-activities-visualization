id: github_activities_ingestion
namespace: zoomcamp
description: |
  Pipeline to upload github archive files to Google Cloud a GCS Bucket

variables:
  file: "{{trigger.date | date('yyyy-MM-dd-HH')}}.json"
  data: "{{outputs['extract']['outputFiles'][(trigger.date | date('yyyy-MM-dd-HH')) ~ '.json']}}"
  gcs_file: "gs://{{kv('GCP_BUCKET_NAME')}}/{{trigger.date | date('yyyy-MM-dd-HH')}}.json"
  table: "{{ 'github_' ~ (trigger.date | date('yyyy-MM-dd-HH')) }}"

tasks:
  - id: extract
    type: io.kestra.plugin.scripts.shell.Commands
    outputFiles:
      - "{{render(vars.file)}}"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    commands:
      - wget -qO- https://data.gharchive.org/{{render(vars.file)}}.gz | gunzip > {{render(vars.file)}}
  
  - id: upload_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    from: "{{render(vars.data)}}"
    to: "{{render(vars.gcs_file)}}"

  - id: bq_mastertable
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE TABLE IF NOT EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.github`
      (
          unique_row_id BYTES OPTIONS (description = 'A unique identifier for the github record, generated by hashing key attributes.'),
          filename STRING OPTIONS (description = 'The source filename from which the github activity data was loaded.'),
          id STRING,
          public BOOL,
          repo STRING, 
          payload STRING,,
          actor STRING,
          org STRING,
          created_at TIMESTAMP,
          type STRING
      )

  - id: bq_filetable_table_ext
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE OR REPLACE EXTERNAL TABLE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.github_filetable_ext`
      OPTIONS (
          format = 'JSON',
          uris = ['{{render(vars.gcs_file)}}']
      );

  - id: bq_filetable_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE OR REPLACE TABLE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.github_filetable` AS
      SELECT
        MD5(CONCAT(
          COALESCE(CAST(id AS STRING), ""),
          COALESCE(CAST(public AS STRING), ""),
          COALESCE(TO_JSON_STRING(repo), ""),
          COALESCE(TO_JSON_STRING(payload), ""),
          COALESCE(TO_JSON_STRING(actor), ""),
          COALESCE(TO_JSON_STRING(org), ""),
          COALESCE(CAST(created_at AS STRING), ""),
          COALESCE(CAST(type AS STRING), "")
        )) AS unique_row_id,
        "{{render(vars.file)}}" AS filename,
        CAST(id AS STRING) as id,
        CAST(public AS BOOL) AS public,
        TO_JSON_STRING(repo) AS repo,
        TO_JSON_STRING(payload) AS payload,
        TO_JSON_STRING(actor) AS actor,
        TO_JSON_STRING(org) AS org,
        CAST(created_at AS TIMESTAMP) AS created_at,
        CAST(type AS STRING) as type
      FROM `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.github_filetable_ext`;

  - id: bq_merge
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      MERGE INTO `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.github` T
      USING `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.github_filetable` S
      ON T.unique_row_id = S.unique_row_id
      WHEN NOT MATCHED THEN
        INSERT (unique_row_id, filename, id, public, repo, payload, actor, org, created_at, type)
        VALUES (S.unique_row_id, S.filename, S.id, S.public, S.repo, S.payload, S.actor, S.org, S.created_at, S.type);

  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

triggers:
  - id: schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 15 1 * *" #Run the flow on the 1st day of every month at 15:00 (UTC)

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"